{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsGSFFLjrmL-",
        "outputId": "2d8e91bf-52f3-44b7-d217-287e82e157c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pq9CAEw1rg-5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import Input,Conv2D,Concatenate\n",
        "from keras.models import Model\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wvc0_VfRrg-6"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 32\n",
        "MAX_TRAIN_IMAGES = 400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9JzD3Iorg-7"
      },
      "source": [
        "PRE_REQUISITE FUNCTION TO GENERATE DATA FROM FOLDERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5KIhW-7irg-7"
      },
      "outputs": [],
      "source": [
        "def load_data(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(images=img, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    img = img / 255.0  #scaling between [0,1]\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JSThutVSrg-7"
      },
      "outputs": [],
      "source": [
        "def data_generator(low_light_images):\n",
        "    data = tf.data.Dataset.from_tensor_slices((low_light_images))\n",
        "    data = data.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    data = data.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kf7WB-Prg-7"
      },
      "source": [
        "DATASET PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCUoqMZprg-8",
        "outputId": "592154ee-3da6-4998-c068-8faf0b40b06b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of image_paths: <class 'list'>\n",
            "Type of elements in image_paths: <class 'str'>\n",
            "Type of image_paths: <class 'list'>\n",
            "Type of elements in image_paths: No elements\n",
            "Warning: Empty image paths list passed to data_generator.\n",
            "Train Dataset: <_ParallelMapDataset element_spec=TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)>\n",
            "Validation Dataset: <_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.float32, name=None)>\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the maximum number of training images\n",
        "MAX_TRAIN_IMAGES = 1000  # example value, set it to your actual requirement\n",
        "\n",
        "# Sort and split the images for training and validation\n",
        "train_low_light_images = sorted(glob.glob(\"/content/drive/MyDrive/Train/low/*\"))[:MAX_TRAIN_IMAGES]\n",
        "val_low_light_images = sorted(glob.glob(\"/content/drive/MyDrive/Train/low/*\"))[MAX_TRAIN_IMAGES:]\n",
        "\n",
        "# Load test images\n",
        "test_low_light_images = sorted(glob.glob(\"/content/drive/MyDrive/Train/low/*\"))\n",
        "test_high_light_images = sorted(glob.glob(\"/content/drive/MyDrive/Train/high/*\"))\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_image(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    return image\n",
        "\n",
        "# Assuming data_generator is a function that prepares datasets\n",
        "def data_generator(image_paths):\n",
        "    # Debug: Check types of the elements in image_paths\n",
        "    print(f\"Type of image_paths: {type(image_paths)}\")\n",
        "    print(f\"Type of elements in image_paths: {type(image_paths[0]) if image_paths else 'No elements'}\")\n",
        "\n",
        "    if not image_paths:\n",
        "        print(\"Warning: Empty image paths list passed to data_generator.\")\n",
        "        return tf.data.Dataset.from_tensor_slices([])  # Return an empty dataset\n",
        "\n",
        "    # Ensure paths are strings\n",
        "    image_paths = [str(path) for path in image_paths]\n",
        "\n",
        "    # Create a TensorFlow dataset from the image paths\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Create training and validation datasets\n",
        "train_dataset = data_generator(train_low_light_images)\n",
        "val_dataset = data_generator(val_low_light_images)\n",
        "\n",
        "# Print datasets (this will print the structure, not the actual images)\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Validation Dataset:\", val_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IeYvhYomrg-8"
      },
      "outputs": [],
      "source": [
        "def Build_DCE_NET():\n",
        "    input_img = Input(shape=[None, None, 3])\n",
        "    x1 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(input_img)\n",
        "    x2 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(x1)\n",
        "    x3 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(x2)\n",
        "    x4 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(x3)\n",
        "\n",
        "    int_x1 = Concatenate(axis=-1)([x4, x3])\n",
        "    x5 = layers.Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(int_x1)\n",
        "\n",
        "    int_x2 = Concatenate(axis=-1)([x5, x2])\n",
        "    x6 = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")(int_x2)\n",
        "\n",
        "    int_x3 = Concatenate(axis=-1)([x6, x1])\n",
        "    y = Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(int_x3)\n",
        "\n",
        "    return Model(inputs=input_img, outputs=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVcXP0Efrg-8"
      },
      "source": [
        "CUSTOM LOSS FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "m5Lq6xsbrg-8"
      },
      "outputs": [],
      "source": [
        "def color_constancy_loss(x):\n",
        "    mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n",
        "    jr, jg, jb = (\n",
        "        mean_rgb[:, :, :, 0],\n",
        "        mean_rgb[:, :, :, 1],\n",
        "        mean_rgb[:, :, :, 2],\n",
        "    )\n",
        "    #ji denotes average intensity of ith channel\n",
        "    #pairwise taking squares\n",
        "    diff_rg = tf.square(jr - jg)\n",
        "    diff_rb = tf.square(jr - jb)\n",
        "    diff_gb = tf.square(jb - jg)\n",
        "\n",
        "    L_col = tf.sqrt(tf.square(diff_rg) + tf.square(diff_rb) + tf.square(diff_gb))\n",
        "    return  L_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xKREcF4drg-8"
      },
      "outputs": [],
      "source": [
        "def exposure_loss(x, E=0.6):  # E is the grey level in RGB color generally taken as 0.6 for experiments\n",
        "    x = tf.reduce_mean(x, axis=3, keepdims=True)\n",
        "    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\") #averaging over 16x16 non-overlapping regions\n",
        "    L_exp = tf.reduce_mean(tf.square(mean - E)) #took the mean of all the values\n",
        "    return L_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Bq-Y7VZUrg-8"
      },
      "outputs": [],
      "source": [
        "def illumination_smoothness_loss(x):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    h_x = tf.shape(x)[1]\n",
        "    w_x = tf.shape(x)[2]\n",
        "    count_h = (w_x - 1) * tf.shape(x)[3]\n",
        "    count_w = w_x * (tf.shape(x)[3] - 1)\n",
        "    h_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))\n",
        "    w_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))\n",
        "    batch_size = tf.cast(batch_size, dtype=tf.float32)\n",
        "    count_h = tf.cast(count_h, dtype=tf.float32)\n",
        "    count_w = tf.cast(count_w, dtype=tf.float32)\n",
        "    return 2 * (h_tv / count_h + w_tv / count_w) / batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqvwEImzrg-9"
      },
      "source": [
        "CUSTOM ZERO DCE MODEL ----> MODIFYING IT'S Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "jTYC9baHrg-9"
      },
      "outputs": [],
      "source": [
        "class ZeroDCE(keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dce_model = Build_DCE_NET()\n",
        "\n",
        "    #custom compile function to accomadate custom loss functions\n",
        "    def compile(self, learning_rate, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        #self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(name=\"illumination_smoothness_loss\")\n",
        "        self.color_constancy_loss_tracker = keras.metrics.Mean(name=\"color_constancy_loss\")\n",
        "        self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.illumination_smoothness_loss_tracker,\n",
        "            self.color_constancy_loss_tracker,\n",
        "            self.exposure_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def get_enhanced_image(self, data, output):\n",
        "        r1 = output[:, :, :, :3]\n",
        "        r2 = output[:, :, :, 3:6]\n",
        "        r3 = output[:, :, :, 6:9]\n",
        "        r4 = output[:, :, :, 9:12]\n",
        "        r5 = output[:, :, :, 12:15]\n",
        "        r6 = output[:, :, :, 15:18]\n",
        "        r7 = output[:, :, :, 18:21]\n",
        "        r8 = output[:, :, :, 21:24]\n",
        "        x = data + r1 * (tf.square(data) - data)\n",
        "        x = x + r2 * (tf.square(x) - x)\n",
        "        x = x + r3 * (tf.square(x) - x)\n",
        "        enhanced_image = x + r4 * (tf.square(x) - x)\n",
        "        x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n",
        "        x = x + r6 * (tf.square(x) - x)\n",
        "        x = x + r7 * (tf.square(x) - x)\n",
        "        enhanced_image = x + r8 * (tf.square(x) - x)\n",
        "        return enhanced_image\n",
        "\n",
        "    def call(self, data):\n",
        "        dce_net_output = self.dce_model(data)\n",
        "        return self.get_enhanced_image(data, dce_net_output)\n",
        "\n",
        "    #lossed are calcualted by multiplying them with their corrosponding weights\n",
        "    def compute_losses(self, data, output):\n",
        "        enhanced_image = self.get_enhanced_image(data, output)\n",
        "        loss_illumination = 200 * illumination_smoothness_loss(output)\n",
        "        #loss_spatial_constancy = tf.reduce_mean(\n",
        "            #self.spatial_constancy_loss(enhanced_image, data)\n",
        "        #)\n",
        "        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n",
        "        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n",
        "        total_loss = (loss_illumination+ loss_color_constancy+ loss_exposure)\n",
        "\n",
        "        return {\n",
        "            \"total_loss\": total_loss,\n",
        "            \"illumination_smoothness_loss\": loss_illumination,\n",
        "            \"color_constancy_loss\": loss_color_constancy,\n",
        "            \"exposure_loss\": loss_exposure,\n",
        "        }\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self.dce_model(data)\n",
        "            losses = self.compute_losses(data, output)\n",
        "\n",
        "        gradients = tape.gradient(losses[\"total_loss\"], self.dce_model.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights)) #weights are updated accroding to the gradients\n",
        "\n",
        "        #every loss is updated\n",
        "        self.total_loss_tracker.update_state(losses[\"total_loss\"]) #total loss status updated\n",
        "        self.illumination_smoothness_loss_tracker.update_state(losses[\"illumination_smoothness_loss\"])\n",
        "        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n",
        "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
        "\n",
        "        return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        output = self.dce_model(data)\n",
        "        losses = self.compute_losses(data, output)\n",
        "\n",
        "        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n",
        "        self.illumination_smoothness_loss_tracker.update_state(\n",
        "            losses[\"illumination_smoothness_loss\"]\n",
        "        )\n",
        "        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n",
        "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
        "\n",
        "        return {metric.name: metric.result() for metric in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "uDnP9yJdrg-9"
      },
      "outputs": [],
      "source": [
        "def plot(images,titles,figure_size = (10,10)):\n",
        "    fig = plt.figure(figsize=figure_size)\n",
        "    for i in range(len(images)):\n",
        "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
        "        _fig = plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "MvalysA3rg-9"
      },
      "outputs": [],
      "source": [
        "def calculate_psnr(original_image, enhanced_image):\n",
        "  # Convert the images to numpy arrays.\n",
        "  original_image = np.array(original_image)\n",
        "  enhanced_image = np.array(enhanced_image)\n",
        "  # Calculate the mean squared error (MSE) between the two images.\n",
        "  mse = np.mean((original_image - enhanced_image) ** 2)\n",
        "  # Calculate the peak signal-to-noise ratio (PSNR).\n",
        "  psnr = 10 * np.log10(255 ** 2 / mse)\n",
        "  return psnr"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNMf1ljxZW0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "RgXB7-j9rg-9",
        "outputId": "6634e155-7ad3-4a57-97b1-c6eb57ab6a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-52-d00de7689abc>\", line 46, in train_step\n        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1222, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1184, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'conv2d_49/kernel:0' shape=(3, 3, 3, 32) dtype=float32>), (None, <tf.Variable 'conv2d_49/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_50/kernel:0' shape=(3, 3, 32, 32) dtype=float32>), (None, <tf.Variable 'conv2d_50/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_51/kernel:0' shape=(3, 3, 32, 32) dtype=float32>), (None, <tf.Variable 'conv2d_51/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_52/kernel:0' shape=(3, 3, 32, 32) dtype=float32>), (None, <tf.Variable 'conv2d_52/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_53/kernel:0' shape=(3, 3, 64, 32) dtype=float32>), (None, <tf.Variable 'conv2d_53/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_54/kernel:0' shape=(3, 3, 64, 32) dtype=float32>), (None, <tf.Variable 'conv2d_54/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_55/kernel:0' shape=(3, 3, 64, 24) dtype=float32>), (None, <tf.Variable 'conv2d_55/bias:0' shape=(24,) dtype=float32>)).\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-0ca1e5ec78be>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage_Enhancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-d00de7689abc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdce_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdce_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-52-d00de7689abc>\", line 46, in train_step\n        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1222, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1184, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'conv2d_49/kernel:0' shape=(3, 3, 3, 32) dtype=float32>), (None, <tf.Variable 'conv2d_49/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_50/kernel:0' shape=(3, 3, 32, 32) dtype=float32>), (None, <tf.Variable 'conv2d_50/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_51/kernel:0' shape=(3, 3, 32, 32) dtype=float32>), (None, <tf.Variable 'conv2d_51/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_52/kernel:0' shape=(3, 3, 32, 32) dtype=float32>), (None, <tf.Variable 'conv2d_52/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_53/kernel:0' shape=(3, 3, 64, 32) dtype=float32>), (None, <tf.Variable 'conv2d_53/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_54/kernel:0' shape=(3, 3, 64, 32) dtype=float32>), (None, <tf.Variable 'conv2d_54/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'conv2d_55/kernel:0' shape=(3, 3, 64, 24) dtype=float32>), (None, <tf.Variable 'conv2d_55/bias:0' shape=(24,) dtype=float32>)).\n"
          ]
        }
      ],
      "source": [
        "Image_Enhancer = ZeroDCE()\n",
        "Image_Enhancer.compile(learning_rate = 1e-4)\n",
        "history = Image_Enhancer.fit(train_dataset, validation_data=val_dataset, epochs=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzaH7US2rg-9"
      },
      "outputs": [],
      "source": [
        "def low_to_high_light(original_image):\n",
        "    image = keras.utils.img_to_array(original_image)\n",
        "    image = image.astype(\"float32\") / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    output_image = Image_Enhancer(image)\n",
        "    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n",
        "    output_image = Image.fromarray(output_image.numpy())\n",
        "    return output_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying Results for 10 images"
      ],
      "metadata": {
        "id": "WVMJLT8VwFh0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI3m56wqrg-9"
      },
      "outputs": [],
      "source": [
        "for val_image_file in test_low_light_images[:10]:\n",
        "    original_image = Image.open(val_image_file)\n",
        "    enhanced_image = low_to_high_light(original_image)\n",
        "    plot(\n",
        "        [original_image,enhanced_image],\n",
        "        [\"Original\",\"Enhanced\"],\n",
        "        (20, 12),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7Wz9jKDrg-9"
      },
      "outputs": [],
      "source": [
        "psnr_ratio = []\n",
        "for i in range(len(test_low_light_images)):\n",
        "    low_light_image = Image.open(test_low_light_images[i])\n",
        "    enhanced_image = low_to_high_light(low_light_image)\n",
        "    high_light_image = Image.open(test_high_light_images[i])\n",
        "    psnr = calculate_psnr(high_light_image, enhanced_image)\n",
        "    psnr_ratio.append(psnr)\n",
        "    #print(f\"PSNR for {test_low_light_images[i]}: {psnr}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(psnr_ratio)"
      ],
      "metadata": {
        "id": "c7THYKlRvCBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}